{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0ed94b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0eff668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import round,col,desc,count\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e49cb4",
   "metadata": {},
   "source": [
    "# Session Creation\n",
    "<br>\n",
    "https://www.kaggle.com/code/rashid60/ml-with-pyspark-predicting-flight-delays/notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8e897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 13:21:12 WARN Utils: Your hostname, Jaminurs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.4 instead (on interface en0)\n",
      "22/12/04 13:21:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 13:21:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/04 13:21:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.executor.memory\", \"70g\")\\\n",
    "     .config(\"spark.driver.memory\", \"50g\").config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "     .config(\"spark.memory.offHeap.size\",\"16g\").appName(\"Flight_delay\").getOrCreate()\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder.appName('Flight_delay').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45152e70",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e4a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Spark_frame  = spark.read.option('recursiveFileLookup','True').option(\"header\", \"true\")\\\n",
    "                        .csv(\"dataverse_files/*\")\n",
    "# Flight_Spark_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71405979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Year', 'string'), ('Month', 'string'), ('DayofMonth', 'string'), ('DayOfWeek', 'string'), ('DepTime', 'string'), ('CRSDepTime', 'string'), ('ArrTime', 'string'), ('CRSArrTime', 'string'), ('UniqueCarrier', 'string'), ('FlightNum', 'string'), ('TailNum', 'string'), ('ActualElapsedTime', 'string'), ('CRSElapsedTime', 'string'), ('AirTime', 'string'), ('ArrDelay', 'string'), ('DepDelay', 'string'), ('Origin', 'string'), ('Dest', 'string'), ('Distance', 'string'), ('TaxiIn', 'string'), ('TaxiOut', 'string'), ('Cancelled', 'string'), ('CancellationCode', 'string'), ('Diverted', 'string'), ('CarrierDelay', 'string'), ('WeatherDelay', 'string'), ('NASDelay', 'string'), ('SecurityDelay', 'string'), ('LateAircraftDelay', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(Flight_Spark_frame.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82ce90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_Spark_frame  = spark.read.option(\"header\", \"true\").csv(\"carriers.csv\")\n",
    "plane_Spark_frame  = spark.read.option(\"header\", \"true\").csv(\"plane-data.csv\")\n",
    "airport_Spark_frame  = spark.read.option(\"header\", \"true\").csv(\"airports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d3ae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Code', 'string'), ('Description', 'string')]\n",
      "[('tailnum', 'string'), ('type', 'string'), ('manufacturer', 'string'), ('issue_date', 'string'), ('model', 'string'), ('status', 'string'), ('aircraft_type', 'string'), ('engine_type', 'string'), ('Pyear', 'string')]\n",
      "[('iata', 'string'), ('airport', 'string'), ('city', 'string'), ('state', 'string'), ('country', 'string'), ('lat', 'string'), ('long', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(carrier_Spark_frame.dtypes)\n",
    "plane_Spark_frame = plane_Spark_frame.withColumnRenamed(\"year\",\"Pyear\")\n",
    "print(plane_Spark_frame.dtypes)\n",
    "print(airport_Spark_frame.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de73707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tailnum: string, type: string, manufacturer: string, issue_date: string, model: string, status: string, aircraft_type: string, engine_type: string, Pyear: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plane_Spark_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785d4c4",
   "metadata": {},
   "source": [
    "# String Transfromation to Integer For Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14221dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Spark_frame = Flight_Spark_frame.withColumn(\"WeatherDelay\",col(\"WeatherDelay\").cast(IntegerType())) \\\n",
    "    .withColumn(\"NASDelay\",col(\"NASDelay\").cast(IntegerType())) \\\n",
    "    .withColumn(\"SecurityDelay\",col(\"SecurityDelay\").cast(IntegerType()))\\\n",
    "    .withColumn(\"LateAircraftDelay\",col(\"LateAircraftDelay\").cast(IntegerType()))\\\n",
    "    .withColumn(\"CarrierDelay\",col(\"CarrierDelay\").cast(IntegerType()))\\\n",
    "    .withColumn(\"ArrDelay\",col(\"ArrDelay\").cast(IntegerType()))\\\n",
    "    .withColumn(\"DepDelay\",col(\"DepDelay\").cast(IntegerType()))\\\n",
    "    .withColumn(\"Month\",col(\"Month\").cast(IntegerType()))\\\n",
    "    .withColumn(\"Year\",col(\"Year\").cast(IntegerType()))\\\n",
    "    .withColumn(\"DayOfWeek\",col(\"DayOfWeek\").cast(IntegerType()))\\\n",
    "    .withColumn(\"DayofMonth\",col(\"DayofMonth\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f40217",
   "metadata": {},
   "source": [
    "# MERGE flight data with plane and airport "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3af7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_flight_data = Flight_Spark_frame.join(plane_Spark_frame,\"TailNum\",\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43edfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departure Merge takes origin information \n",
    "plane_flight_dep_airport_data = plane_flight_data.withColumn(\"Origin\", col(\"Origin\")).\\\n",
    "                                join(airport_Spark_frame.withColumn(\"Origin\", col(\"iata\")), on=\"Origin\")\n",
    "# Arrival Merge takes destination information \n",
    "plane_flight_arr_airport_data = plane_flight_data.withColumn(\"Dest\", col(\"Dest\")).\\\n",
    "                                join(airport_Spark_frame.withColumn(\"Dest\", col(\"iata\")), on=\"Dest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88550a0a",
   "metadata": {},
   "source": [
    "# Label Creation for Prediction Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160dec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_delay_Lab_f = plane_flight_dep_airport_data.withColumn('label', (plane_flight_dep_airport_data.DepDelay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "# dep_delay_Lab_f.select(col(\"label\")).where(dep_delay_Lab_f.label == 1).show(5)\n",
    "\n",
    "# dep_delay_Lab_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836fe181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_delay_Lab_f.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b641201",
   "metadata": {},
   "source": [
    "# Some String col to Index creation for the model\n",
    "<br>\n",
    "String column that are string neumeric values that does not require the transformation but \n",
    "<br>Values like origin string \"ORD\" requires to transform to some equivalent indexer by mapping the following code is doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5121bebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flights_indexed = StringIndexer(inputCol='UniqueCarrier', outputCol='UniqueCarrier_idx').fit(dep_delay_Lab_f).transform(dep_delay_Lab_f)\n",
    "\n",
    "# Repeat the process for org column\n",
    "flights_indexed = StringIndexer(inputCol='Origin', outputCol='Origin_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "# flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73412af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following line transforming distance to integer as it will be used for ML model\n",
    "flights_indexed = flights_indexed.withColumn(\"Distance\",col(\"Distance\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009bd75",
   "metadata": {},
   "source": [
    "# Feature Vector Creation for Departure Delay Prediction Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e40fbade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------+\n",
      "|features                    |DepDelay|\n",
      "+----------------------------+--------+\n",
      "|[1.0,1.0,1.0,6.0,479.0,10.0]|10      |\n",
      "|[1.0,1.0,1.0,6.0,647.0,9.0] |9       |\n",
      "|[1.0,1.0,1.0,6.0,647.0,3.0] |3       |\n",
      "|[1.0,1.0,1.0,6.0,480.0,0.0] |0       |\n",
      "|[1.0,1.0,1.0,6.0,480.0,2.0] |2       |\n",
      "+----------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['Month', 'DayofMonth', 'DayOfWeek',\n",
    "'UniqueCarrier_idx', 'Distance', 'DepDelay'], outputCol='features')\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.setHandleInvalid(\"skip\").transform(flights_indexed)\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'DepDelay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15f46dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'features'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_assembled['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e7843",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1c7f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 13:24:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999771516368166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights_assembled.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fa1a0",
   "metadata": {},
   "source": [
    "# Logistic Regression Model \n",
    "<br>\n",
    "Takes huge time to train. Timer can be added to get the time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9dcdee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                        (0 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/27 21:37:55 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/11/27 21:37:55 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>                                                       (0 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 04:54:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/28 04:54:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/28 04:54:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:>                                                       (0 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 04:54:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/28 04:55:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:===>                                                    (1 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 04:55:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:===>                                                    (1 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 04:56:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:======>                                                 (2 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 04:56:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:=========>                                              (3 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 04:57:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/28 04:57:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    0|       0.0|7131354|\n",
      "|    1|       1.0|1544219|\n",
      "+-----+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801c738",
   "metadata": {},
   "source": [
    "# Precision and Recall based Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "371d5acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 1.00\n",
      "recall    = 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2aa0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dtreeviz\n",
    "# from dtreeviz.models.spark_decision_tree import ShadowSparkTree\n",
    "# from dtreeviz import trees\n",
    "# features = ['Month', 'DayofMonth', 'DayOfWeek','UniqueCarrier_idx', 'Distance', 'DepDelay']\n",
    "# target = \"DepDelay\"\n",
    "# spark_dtree = ShadowSparkTree(tree_model, flights_assembled[features], flights_assembled[target], feature_names= features, target_name=target, class_names=[0, 1])\n",
    "# trees.dtreeviz(spark_dtree, fancy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9cd89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
